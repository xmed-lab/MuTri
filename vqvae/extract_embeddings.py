import pickle
from argparse import ArgumentParser
from pathlib import Path

import lmdb
import torch
import numpy as np
from tqdm import tqdm

from vqvae.model import VQVAE
from utils import CTDataModule


GPU = torch.device('cuda')

def extract_samples(model, dataloader):
    model.eval()
    model.to(GPU)

    with torch.no_grad():
        for sample, _ in dataloader:
            sample = sample.to(GPU)
            *_, encoding_idx = zip(*model.encode(sample))
            yield encoding_idx


def get_output_abspath(checkpoint_path: Path, output_path: Path, output_name: str = '') -> str:
    assert output_path.is_dir()
    assert checkpoint_path.is_file()
    checkpoint_path = checkpoint_path.resolve()

    if output_name == '':
        if 'version' in checkpoint_path.parts[-3]:
            print("Assuming checkpoint was generated by slurm")
            output_name = checkpoint_path.parts[-3] + '_' + checkpoint_path.stem 
        else:
            output_name = checkpoint_path.stem
        output_name += '.lmdb'

    return str((output_path / output_name).resolve())


def main(args):
    if args.checkpoint_path is not None:
        model = VQVAE.load_from_checkpoint(str(args.checkpoint_path))
    else:
        model = VQVAE()

    datamodule = CTDataModule(
        path=args.dataset_path,
        batch_size=1,
        train_frac=1,
        num_workers=5,
        rescale_input=(256,256,128)
    )
    datamodule.setup()
    dataloader = datamodule.train_dataloader()

    db = lmdb.open(
        get_output_abspath(args.checkpoint_path, args.output_path, args.output_name),
        map_size=int(1e12),
        max_dbs=model.n_bottleneck_blocks
    )

    sub_dbs = [db.open_db(str(i).encode()) for i in range(model.n_bottleneck_blocks)]
    with db.begin(write=True) as txn:
        # Write root db metadata
        txn.put(b"num_dbs", str(model.n_bottleneck_blocks).encode())
        txn.put(b"length",  str(len(dataloader)).encode())
        txn.put(b"num_embeddings", pickle.dumps(np.asarray(model.num_embeddings)))

        for i, sample_encodings in tqdm(enumerate(extract_samples(model, dataloader)), total=len(dataloader)):
            for sub_db, encoding in zip(sub_dbs, sample_encodings):
                txn.put(str(i).encode(), pickle.dumps(encoding.cpu().numpy()), db=sub_db)

    db.close()

if __name__ == '__main__':
    parser = ArgumentParser()

    parser.add_argument("--output-path", type=Path, default=Path("."))
    parser.add_argument("--output-name", type=str, default='', help="default: takes over the checkpoint name with .lmdb file ext")
    parser.add_argument("--checkpoint-path", type=Path, required=True)
    parser.add_argument("--dataset-path", type=Path, required=True)

    args = parser.parse_args()

    main(args)